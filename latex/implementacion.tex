\subsection{Eliminación Gaussiana}

\subsubsection{Descripción del método}

El método de Eliminación Gaussiana consiste en una serie de pasos que permiten resolver un sistema de ecuaciones lineales de, en principio, $n$ ecuaciones y $n$ variables.

Sea A $\in \mathbb{R}^{n \times\ n}$ la matriz tal que el elemento en la fila $i$ y columna $j$ ($a_{i,j}$) representa el coeficiente de la variable $j$ en la ecuación $i$.
Y sea b $\in \mathbb{R}^{n}$ el vector tal que el elemento en la fila $i$ ($b_{i}$) representa el termino independiente en la ecuación $i$.

Podemos dividir el método en 2 partes centrales:
\begin{enumerate}
    \item Llevar la matriz A a una forma \textbf{Triangular Superior}, es decir, una matriz equivalente a A tal que tiene ceros debajo de los elementos de la diagonal. El siguiente pseudocódigo muestra como es el algoritmo para realizar esta tarea:

\begin{lstlisting}
Para j desde 0 hasta n-1 hacer:
    Poner pivote = A[j][j]
    Para i desde j+1 hasta n-1 hacer:
        Poner coeficiente = A[i][j] / pivote
        Poner A[i][j] = 0
        Para k desde j+1 hasta n-1 hacer:
            Poner A[i][k] = A[i][k] - coeficiente * A[j][k]
        Fin para
        b[i] = b[i] - coeficiente * b[j]
    Fin para
Fin para
\end{lstlisting}

		Notese que no validamos que la variable ``pivote'' sea distinta de cero. Esto es así ya que por la forma en la que se modeló el problema el pivote siempre es distinto de cero.

    \item \textbf{Resolver el sistema equivalente}. Para esto, vamos a utilizar que la matriz es Triangular Superior. La idea es empezar despejando el valor de la $n$-ésima variable, luego usar este valor para despejar la $(n-1)$-ésima variable, y así sucesivamente hasta la primera variable. En pseudocódigo:

\begin{lstlisting}
Poner X = vector de n elementos
Para i desde n-1 hasta 0 hacer:
    Poner X[i] = b[i]
    Para j desde i+1 hasta n-1 hacer:
        Poner X[i] = X[i] - U[i][j] * X[j]
    Fin para
    Poner X[i] = X[i] / U[i][i]
Fin para
\end{lstlisting}

		Donde $U$ es la matriz que calculamos en el paso 1.

  \end{enumerate}


\subsubsection{Utilizando que la matriz es Banda}

Si miramos la matriz con la cual representamos el modelo del problema, podemos ver que alrededor de los elementos de la diagonal hay una ``banda'' de tamaño $2n$.
Es decir, si quisieramos poner elementos debajo del elemento $a_{i,i}$, nos bastaría con modificar las filas desde $i+1$ hasta $i+2n+1$, ya que $\forall\ a_{j,i},\ j> i+2n+1 \implies a_{j,i} = 0$.

Usando esto podemos optimizar significativamente el primer paso de la Eliminación Gaussiana, que consiste en hallar la matriz equivalente Triangular Superior. El pseudocódigo es el siguiente:

\begin{lstlisting}
Para j desde 0 hasta n-1 hacer:
    Poner pivote = A[j][j]
    Poner inicioBanda = max(i+1, n)
    Poner finBanda = min(n, inicioBanda + n)
    Para i desde inicioBanda hasta finBanda hacer:
        Si A[i][j] != 0 hacer:
            Poner coeficiente = A[i][j] / pivote
            Poner A[i][j] = 0
            Para k desde j+1 hasta n-1 hacer:
                Poner A[i][k] = A[i][k] - coeficiente * A[j][k]
            Fin para
            b[i] = b[i] - coeficiente * b[j]
        Fin si
    Fin para
Fin para
\end{lstlisting}

\subsection{Factorización LU}

Dada la matriz A $\in \mathbb{R}^{n \times\ n}$, los vectores x $\in \mathbb{R}^{n}$ y b $\in \mathbb{R}^{n}$ y la ecuación $Ax = b$, esta técnica de resolución descompone la matriz en cuestión de la siguiente manera:

\begin{center}
    $A = LU$ con L,U $\in \mathbb{R}^{n \times\ n}$\\
    L es triangular inferior con 1's en los elementos de su diagonal\\
    U es triangular superior\\
    Y por lo tanto, $Ux = y$, $Ly = b$ reemplazando la ecuación original
\end{center}

Ahora, como vimos en las clases prácticas y teóricas, estas dos matrices son únicas y la forma de obtenerla es haciendo los pasos de eliminación gaussiana (descripta anteriormente) y guardarnos los coeficientes por los cuáles vamos multiplicando las filas para triangular las de abajo. A modo de ejemplo:

\newenvironment{spmatrix}[1]
 {\def\mysubscript{#1}\mathop\bgroup\begin{pmatrix}}
 {\end{pmatrix}\egroup_{\textstyle\mathstrut\mysubscript}}

\newcommand*{\temp}[1]{\multicolumn{1}{c|}{#1}}

\begin{equation*}
\begin{spmatrix}{A}
    5 & −1 & 2 \\
    10 & 3 & 7 \\
    15 & 17 & 19
\end{spmatrix}
=
\begin{spmatrix}{L}
    1 & 0 & 0 \\
    2 & 1 & 0 \\
    3 & 4 & 1
\end{spmatrix}
\times
\begin{spmatrix}{U}
    5 & −1 & 2 \\
    0 & 5 & 3 \\
    0 & 0 & 1
\end{spmatrix}
\end{equation*}

En términos de espacio, guardar las matrices L y U se puede hacer en una 'misma' matriz si se la interpreta de forma diferente. Lo que hay que tener en cuenta que la única parte relevante de U es de la diagonal para arriba (con la diagonal inclusive), pero de L ya sabemos que la diagonal contiene 1's y solo nos interesa saber la parte de abajo. Entonces la matriz anterior se podría guardar así en memoria:

\begin{equation*}
\begin{spmatrix}{L}
    1 & 0 & 0 \\
    2 & 1 & 0 \\
    3 & 4 & 1
\end{spmatrix}
\begin{spmatrix}{U}
    5 & −1 & 2 \\
    0 & 5 & 3 \\
    0 & 0 & 1
\end{spmatrix}
\leftarrow
\left(
\begin{array}{cccc}
5 & -1 & 2 \\ \cline{1-1}
\temp{2} & 5 & 3 \\ \cline{2-2}
3 & \temp{4} & 1
\end{array}
\right)
\end{equation*}

Además, como ya demostramos que la matriz se puede triangular mediante eliminación gaussiana sin pivoteo, sabemos que la factorización LU existe (y no tenemos que tomar matrices de permutación). \\
Teniendo todo eso en cuenta, encontramos la factorización (con el formato explicado antes) con este procedimiento:

\newpage
\begin{lstlisting}
Sea A la matriz que queremos factorizar
Sea n la cantidad de filas (y columnas, porque es cuadrada) de A
Sea M una matriz de n*n (donde guardaremos la respuesta)

FactorizarLU(A, n, M)
    Copiar la primer fila de A hacia M
    Para i desde 0 hasta n-2
        Para j desde i+1 hasta n-1
            Poner c = A[j][i]/A[i][i]
            Poner M[j][i] = c
            Para k desde i+1 hasta n-1
                M[j][k]-= c*A[i][k]
    devolver M
\end{lstlisting}

Ya teniendo la factorización LU de A, solo quedaría resolver las ecuaciones $Ux = y$, $Ly = b$ adaptando los algoritmos clásicos de triangulación de sistemas lineales para que tomen en cuenta el formato con el cual guardamos la factorización LU de A en una sola matriz:

\begin{lstlisting}
Sean A, n y M definidos como el pseudocodigo anterior
Sea b el vector de n elementos correspondiente a la ecuacion a resolver

ResolverSistema(A,n,b)
    M = Crear matriz cuadrada de n*n
    FactorizarLU(A,n,M)
    y = Crear vector de n elementos
    ResolverTriangularInferiorLU(M,b,n,y) \\ Ly = b
    x = Crear vector de n elementos
    ResolverTriangularSuperiorLU(M,y,n,x) \\ Ux = y
    devolver x

ResolverTriangularInferiorLU(M,y,n,x)
    Para i desde 0 hasta n-1
        Poner x[i] = b[i]
        Para j desde 0 hasta i-1
            Poner x[i] = x[i] - M[i][j] * x[j]
        // la diagonal es 1, asi que no hay que dividir por nada

ResolverTriangularSuperiorLU(M,b,n,y) \\ seria igual al procedimiento de la seccion 4.1.1.2
    Para i desde n-1 hasta 0
        Poner x[i] = b[i]
        Para j desde i+1 hasta n-1
            Poner x[i] = x[i] - M[i][j] * x[j]
        Poner x[i] = x[i] / M[i][i]
\end{lstlisting}

Finalmente, es importante aclarar que la ventaja de utilizar la factorización LU de una matriz para resolver sistemas de ecuaciones lineales radica en que una vez computada la factorización, solo queda resolver sistemas triangulares. Es decir, si bien encontrar la factorización LU no es una operación barata, una vez que la obtenemos, solo nos queda resolver las ecuaciones $Ux = y$, $Ly = b$ donde U y L son triangulares. Por lo tanto, una vez que calculamos L y U no hace falta hacer eliminación gaussiana devuelta aunque cambiemos el 'b', a diferencia de la eliminación gaussiana normal.

\subsection{Determinación de la Isoterma}

Recordemos que nuestra discretización particiona una sección circular del Alto Horno de la siguiente forma:
 \begin{itemize}
 	\item $0 = \theta_0 < \theta_1 < ... < \theta_n = 2\pi$ en $n$ \'angulos discretos, y
 	\item $r_i = r_0 < r_1 < ... < r_m = r_e$ en $m+1$ radios discretos
 \end{itemize}

Luego, para cada ángulo $j$ tenemos los puntos: $t_{i,j}$ con $0 \leq i \leq m$.

Entonces, hallar la isoterma $C$ equivale a, para cada ángulo $j$, hallar el radio $r_C$ tal que $T(r_C, \theta_j) = C$.

\subsubsection{Promedio simple}

Este método consiste en, dado un ángulo $j$, buscar un punto $t_{i,j}$ en la solución del sistema tal que $t_{i,j} \leq C \leq t_{i+1,j}$.

Una vez hallado este punto, tenemos que $r_C = \frac{r_i + r_{i+1}}{2}$.

\subsubsection{Búsqueda binaria mediante sistemas de ecuaciones}

Lo que hacemos en este método es, una vez halladas las temperaturas del alto horno según nuestra discretización, sub-discretizar los radios hasta encontrar la isoterma. Esto se realiza encontrando, para cada angulo, dos radios entre los cuáles está la isoterma. Sean $r_{i1}$ y $r_{i2}$ estos radios, creamos un nuevo sistema de ecuaciones con tres radios: los dos anteriores (de los cuales ya sabemos su temperatura) y el radio medio entre esos dos (cuya temperatura queremos hallar). De alguna manera se puede ver como un nuevo horno discretizado con 3 radios, $r_{i1}$ y $r_{i2}$ y el del medio. Este procedimiento se realiza varias veces (tomando radios medios todo el tiempo) asi convergiendo a la isoterma de la misma forma que en el algoritmo de Búsqueda Binaria. Es decir, ya teniendo $t_{i,j}$ para todo i,j con $0 \leq i \leq m$ y $0 \leq j \leq n-1$, hacemos lo siguiente:

\begin{lstlisting}[mathescape=true]
Sea PRECISION = 0.00001 \\ valor configurable, en este caso el mismo que nuestro codigo

CalcularIsotermaBinaria
    Poner solucion = Crear vector de n elementos (para contener el radio en el que esta la isoterma en cada angulo)
    Para j desde 0 hasta n-1
        Poner i2 = el primer radio que cumple $t_{i2,j}$ < isoterma
        Poner i1 = i2-1 \\tener en cuenta que a menores radio, mayores temperaturas
        Poner a2 = $r_{i2}$
        Poner a1 = $r_{i1}$
        Mientras (Valor Absoluto($T(a1,\theta_j)$-isoterma) > PRECISION)
            Poner ah = (a1+a2)/2
            Hallar $T(ah,\theta_j)$
            Si ($T(ah,\theta_j)$ < isoterma) {
                Poner a2 = ah
            Sino
                Poner a1 = ah
            }
        Poner solucion[j] = a1
    devolver solucion
\end{lstlisting}


\subsubsection{Regresión lineal (Linear fit)}

Este método utiliza el algoritmo de regresión lineal para, dado un ángulo $j$, y usando todos los puntos $t_{i,j}$ con $0 \leq i \leq m$, hallar una función lineal que aproxime dichos puntos lo mejor posible.
Como la función que estamos buscando es lineal, es de la forma: $y(x) = a + bx$, donde $b$ es el coeficiente principal, $a$ el termino independiente, $x$ es un radio sobre el ángulo $j$ e $y(x)$ es la temperatura para dicho radio.

Luego, el algoritmo de regresion lineal basicamente utiliza la minimización de la suma de las distancias al cuadradado desde los puntos a la función lineal. Esto se logra calculando la derivada con respecto a $a$ y $b$ y fijando estos en cero.

Entonces, si definimos:

$$\overline{x} = \frac{1}{m}\sum_{i=0}^{m}{r_i} \quad\quad\quad \overline{y} = \frac{1}{m}\sum_{i=0}^{m}{t_{i,j}}$$

$$S_x = \sum_{i=0}^{m}{(r_i - \overline{x})^2} \quad\quad\quad S_{xy} = \sum_{i=0}^{m}{(r_i - \overline{x})(t_{i,j} - \overline{y})}$$

Tenemos que:

$$b = \frac{S_{xy}}{S_x}  \quad\quad\quad a = \overline{y} - b\overline{x}$$

Una vez obtenidos $a$ y $b$, para hallar la isoterma $C$ en el ángulo $j$, basta con calcular:

$$r_C = |C - a|/b$$

En pseudocódigo:

\begin{lstlisting}[mathescape=true]
Poner solucion = vector de n elementos
Para j desde 0 hasta n hacer:
    Poner avgX = 0
    Poner avgY = 0
    Para i desde 0 hasta m hacer:
        Poner avgX = avgX + $r_i$
        Poner avgY = avgY + $t_{i,j}$
    Fin para
    Poner avgX = avgX / m
    Poner avgY = avgY / m
    Poner numerador = 0
    Poner denominador = 0
    Para i desde 0 hasta m hacer:
        Poner numerador = numerador + ($r_i$ - avgX) * ($t_{i,j}$ - avgY)
        Poner denominador = denominador + ($r_i$ - avgX) * ($r_i$ - avgX)
    Fin para
    Si denominador == 0 hacer:
        Poner denominador = 1
    Fin si
    Poner coeficiente = numerador / denominador
    Poner independiente = avgY - slope * coeficiente
    Poner solucion[j] = abs(C - independiente) / coeficiente
Fin para
\end{lstlisting}

\subsection{Evaluación del peligro de la estructura}

Una vez obtenida la isoterma $C$, queremos evaluar la peligrosidad de la estructura en función de la distancia de la isoterma a la pared externa del horno. En este sentido, estamos asumiendo que la temperatura $C$ es elevada y que mientras más cercana está la temperatura de la pared externa a $C$, entonces más peligrosa es la estructura.

En base a esto, proponemos dos medidas distintas para evaluar la peligrosidad.

\subsubsection{Proximidad porcentual simple}

Para cada ángulo $j$, podemos calcular el coeficiente porcentual $\Delta_j(C) = (r_e - r_C)/(r_e - r_i)$, donde $r_e$ es el radio de la pared externa del horno, $r_i$ el radio de la pared interna, y $r_C$ el radio de la isoterma $C$ para el ángulo $j$.

Notese que $r_i \leq r_C \leq r_e$, y por lo tanto si $r_C = r_i \implies \Delta_j(C) = 1$, y si $r_C = r_e \implies \Delta_j(C) = 0$.

De esta forma, podemos definir un $\varepsilon_C$, con $0 < \varepsilon_C < 1 $, tal que decimos que la estructura se encuentra en peligro si:

$$\varepsilon_C \geq \min\limits_{\substack{1 \leq j \leq n-1}}(\Delta_j(C))$$

\subsubsection{Proximidad porcentual promediada}

En la medida anterior, podría pasar que para un $j'$ dado $\Delta_{j'}(C) < \varepsilon_C$ pero el resto de los $\Delta_j(C)$ sean mayores a $\varepsilon_C$, en cuyo caso, igualmente la estructura sería catalogada como peligrosa.

Entonces, querríamos dar una medida de la peligrosidad de la estructura que tome en cuenta todos los ángulos. Para esto, vamos a tomar el promedio de todos los $\Delta_j(C)$, definidos como en la medida anterior para cada ángulo $j$, y decimos que la estructura se encuentra en peligro si:

$$\Delta(C)= \frac{\sum\limits_{j=1}^{n}{\Delta_j(C)}}{n} \leq  \varepsilon_C$$
